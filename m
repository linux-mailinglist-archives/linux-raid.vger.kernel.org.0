Return-Path: <linux-raid-owner@vger.kernel.org>
X-Original-To: lists+linux-raid@lfdr.de
Delivered-To: lists+linux-raid@lfdr.de
Received: from out1.vger.email (out1.vger.email [IPv6:2620:137:e000::1:20])
	by mail.lfdr.de (Postfix) with ESMTP id 0F518613D30
	for <lists+linux-raid@lfdr.de>; Mon, 31 Oct 2022 19:17:57 +0100 (CET)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S229691AbiJaSRt (ORCPT <rfc822;lists+linux-raid@lfdr.de>);
        Mon, 31 Oct 2022 14:17:49 -0400
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:53722 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S229599AbiJaSRt (ORCPT
        <rfc822;linux-raid@vger.kernel.org>); Mon, 31 Oct 2022 14:17:49 -0400
X-Greylist: delayed 2397 seconds by postgrey-1.37 at lindbergh.monkeyblade.net; Mon, 31 Oct 2022 11:17:46 PDT
Received: from 2.mo583.mail-out.ovh.net (2.mo583.mail-out.ovh.net [178.33.109.111])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id A511465C6
        for <linux-raid@vger.kernel.org>; Mon, 31 Oct 2022 11:17:46 -0700 (PDT)
Received: from player759.ha.ovh.net (unknown [10.109.156.39])
        by mo583.mail-out.ovh.net (Postfix) with ESMTP id F388B20F2C
        for <linux-raid@vger.kernel.org>; Mon, 31 Oct 2022 16:58:03 +0000 (UTC)
Received: from rechte.fr (82-65-133-131.subs.proxad.net [82.65.133.131])
        (Authenticated sender: marc4@rechte.fr)
        by player759.ha.ovh.net (Postfix) with ESMTPSA id BC7A2304B0BCC
        for <linux-raid@vger.kernel.org>; Mon, 31 Oct 2022 16:58:03 +0000 (UTC)
Authentication-Results: garm.ovh; auth=pass (GARM-99G003936a450b-bd63-4f81-bb30-81064634c41d,
                    71E8FAE561B63374669873A88B776FC2EB8801BC) smtp.auth=marc4@rechte.fr
X-OVh-ClientIp: 82.65.133.131
Message-ID: <3ebe5287-67bd-761b-1fef-9bf41ccadd71@rechte.fr>
Date:   Mon, 31 Oct 2022 17:58:03 +0100
MIME-Version: 1.0
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101
 Thunderbird/102.4.1
To:     linux-raid@vger.kernel.org
Content-Language: en-US
From:   =?UTF-8?Q?Marc_Recht=c3=a9?= <marc4@rechte.fr>
Subject: mdadm udev rule does not start mdmonitor systemd unit.
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Ovh-Tracer-Id: 18346257507931528378
X-VR-SPAMSTATE: OK
X-VR-SPAMSCORE: 0
X-VR-SPAMCAUSE: gggruggvucftvghtrhhoucdtuddrgedvgedrudefgdelhecutefuodetggdotefrodftvfcurfhrohhfihhlvgemucfqggfjpdevjffgvefmvefgnecuuegrihhlohhuthemucehtddtnecunecujfgurhepkfffgggfvffhufgtgfesthejredttdefjeenucfhrhhomhepofgrrhgtucftvggthhhtrocuoehmrghrtgegsehrvggthhhtvgdrfhhrqeenucggtffrrghtthgvrhhnpeefgeeiueelfeefteeiffdthefhgeekudettdfhleetueekheehlefggeelheefkeenucfkphepuddvjedrtddrtddruddpkedvrdeihedrudeffedrudefudenucevlhhushhtvghrufhiiigvpedtnecurfgrrhgrmhepihhnvghtpeduvdejrddtrddtrddupdhmrghilhhfrhhomhepoehmrghrtgegsehrvggthhhtvgdrfhhrqedpnhgspghrtghpthhtohepuddprhgtphhtthhopehlihhnuhigqdhrrghiugesvhhgvghrrdhkvghrnhgvlhdrohhrghdpoffvtefjohhsthepmhhoheekfedpmhhouggvpehsmhhtphhouhht
X-Spam-Status: No, score=-1.9 required=5.0 tests=BAYES_00,RCVD_IN_DNSWL_NONE,
        RCVD_IN_MSPIKE_H3,RCVD_IN_MSPIKE_WL,SPF_HELO_NONE,SPF_PASS
        autolearn=ham autolearn_force=no version=3.4.6
X-Spam-Checker-Version: SpamAssassin 3.4.6 (2021-04-09) on
        lindbergh.monkeyblade.net
Precedence: bulk
List-ID: <linux-raid.vger.kernel.org>
X-Mailing-List: linux-raid@vger.kernel.org

Hello,

I have a udev rule and a md127 device with the properties as following.

The mdmonitor service is not started (no trace in systemd journal). 
However I can manually start the service.

I don't know how to further debug.

Thanks

# udevadm info --query=property --name=/dev/md127

DEVPATH=/devices/virtual/block/md127
DEVNAME=/dev/md127
DEVTYPE=disk
DISKSEQ=6
MAJOR=9
MINOR=127
SUBSYSTEM=block
USEC_INITIALIZED=5129215
ID_IGNORE_DISKSEQ=1
MD_LEVEL=raid1
MD_DEVICES=2
MD_METADATA=1.2
MD_UUID=800ee577:652e6fdc:79f6768e:dea2f7ea
MD_DEVNAME=SysRAID1Array1
MD_NAME=linux2:SysRAID1Array1
ID_FS_UUID=x94VGG-7hfP-rn1c-MR53-q6to-QPZR-73eAdq
ID_FS_UUID_ENC=x94VGG-7hfP-rn1c-MR53-q6to-QPZR-73eAdq
ID_FS_VERSION=LVM2 001
ID_FS_TYPE=LVM2_member
ID_FS_USAGE=raid
SYSTEMD_WANTS=mdmonitor.service
SYSTEMD_READY=0
UDISKS_MD_LEVEL=raid1
UDISKS_MD_DEVICES=2
UDISKS_MD_METADATA=1.2
UDISKS_MD_UUID=800ee577:652e6fdc:79f6768e:dea2f7ea
UDISKS_MD_DEVNAME=SysRAID1Array1
UDISKS_MD_NAME=linux2:SysRAID1Array1
UDISKS_MD_DEVICE_dev_nvme0n1p2_ROLE=0
UDISKS_MD_DEVICE_dev_nvme0n1p2_DEV=/dev/nvme0n1p2
UDISKS_MD_DEVICE_dev_nvme1n1p2_ROLE=1
UDISKS_MD_DEVICE_dev_nvme1n1p2_DEV=/dev/nvme1n1p2
DEVLINKS=/dev/md/SysRAID1Array1 
/dev/disk/by-id/md-name-linux2:SysRAID1Array1 
/dev/disk/by-id/lvm-pv-uuid-x94VGG-7hfP-rn1c-MR53-q6to-QPZR-73eAdq 
/dev/disk/by-id/md-uuid-800ee577:652e6fdc:79f6768e:dea2f7ea
TAGS=:systemd:
CURRENT_TAGS=:systemd:

# cat /usr/lib/udev/rules.d/63-md-raid-arrays.rules
# do not edit this file, it will be overwritten on update

SUBSYSTEM!="block", GOTO="md_end"

# handle md arrays
ACTION!="add|change", GOTO="md_end"
KERNEL!="md*", GOTO="md_end"

# partitions have no md/{array_state,metadata_version}, but should not
# for that reason be ignored.
ENV{DEVTYPE}=="partition", GOTO="md_ignore_state"

# container devices have a metadata version of e.g. 'external:ddf' and
# never leave state 'inactive'
ATTR{md/metadata_version}=="external:[A-Za-z]*", 
ATTR{md/array_state}=="inactive", GOTO="md_ignore_state"
TEST!="md/array_state", ENV{SYSTEMD_READY}="0", GOTO="md_end"
ATTR{md/array_state}=="clear*|inactive", ENV{SYSTEMD_READY}="0", 
GOTO="md_end"
ATTR{md/sync_action}=="reshape", ENV{RESHAPE_ACTIVE}="yes"
LABEL="md_ignore_state"

IMPORT{program}="/usr/bin/mdadm --detail --no-devices --export $devnode"
ENV{DEVTYPE}=="disk", ENV{MD_NAME}=="?*", 
SYMLINK+="disk/by-id/md-name-$env{MD_NAME}", 
OPTIONS+="string_escape=replace"
ENV{DEVTYPE}=="disk", ENV{MD_UUID}=="?*", 
SYMLINK+="disk/by-id/md-uuid-$env{MD_UUID}"
ENV{DEVTYPE}=="disk", ENV{MD_DEVNAME}=="?*", SYMLINK+="md/$env{MD_DEVNAME}"
ENV{DEVTYPE}=="partition", ENV{MD_NAME}=="?*", 
SYMLINK+="disk/by-id/md-name-$env{MD_NAME}-part%n", 
OPTIONS+="string_escape=replace"
ENV{DEVTYPE}=="partition", ENV{MD_UUID}=="?*", 
SYMLINK+="disk/by-id/md-uuid-$env{MD_UUID}-part%n"
ENV{DEVTYPE}=="partition", ENV{MD_DEVNAME}=="*[^0-9]", 
SYMLINK+="md/$env{MD_DEVNAME}%n"
ENV{DEVTYPE}=="partition", ENV{MD_DEVNAME}=="*[0-9]", 
SYMLINK+="md/$env{MD_DEVNAME}p%n"

IMPORT{builtin}="blkid"
OPTIONS+="link_priority=100"
OPTIONS+="watch"
ENV{ID_FS_USAGE}=="filesystem|other|crypto", ENV{ID_FS_UUID_ENC}=="?*", 
SYMLINK+="disk/by-uuid/$env{ID_FS_UUID_ENC}"
ENV{ID_FS_USAGE}=="filesystem|other", ENV{ID_PART_ENTRY_UUID}=="?*", 
SYMLINK+="disk/by-partuuid/$env{ID_PART_ENTRY_UUID}"
ENV{ID_FS_USAGE}=="filesystem|other", ENV{ID_FS_LABEL_ENC}=="?*", 
SYMLINK+="disk/by-label/$env{ID_FS_LABEL_ENC}"

ENV{MD_LEVEL}=="raid[1-9]*", ENV{SYSTEMD_WANTS}+="mdmonitor.service"

# Tell systemd to run mdmon for our container, if we need it.
ENV{MD_LEVEL}=="raid[1-9]*", ENV{MD_CONTAINER}=="?*", 
PROGRAM="/usr/bin/readlink $env{MD_CONTAINER}", ENV{MD_MON_THIS}="%c"
ENV{MD_MON_THIS}=="?*", PROGRAM="/usr/bin/basename $env{MD_MON_THIS}", 
ENV{SYSTEMD_WANTS}+="mdmon@%c.service"
ENV{RESHAPE_ACTIVE}=="yes", PROGRAM="/usr/bin/basename 
$env{MD_MON_THIS}", ENV{SYSTEMD_WANTS}+="mdadm-grow-continue@%c.service"

LABEL="md_end"

