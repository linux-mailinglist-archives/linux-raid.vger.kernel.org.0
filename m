Return-Path: <linux-raid-owner@vger.kernel.org>
X-Original-To: lists+linux-raid@lfdr.de
Delivered-To: lists+linux-raid@lfdr.de
Received: from out1.vger.email (out1.vger.email [IPv6:2620:137:e000::1:20])
	by mail.lfdr.de (Postfix) with ESMTP id 0422A676BB1
	for <lists+linux-raid@lfdr.de>; Sun, 22 Jan 2023 09:53:06 +0100 (CET)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S229815AbjAVIxD (ORCPT <rfc822;lists+linux-raid@lfdr.de>);
        Sun, 22 Jan 2023 03:53:03 -0500
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:59822 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S229480AbjAVIxC (ORCPT
        <rfc822;linux-raid@vger.kernel.org>); Sun, 22 Jan 2023 03:53:02 -0500
Received: from mallaury.nerim.net (smtp-100-sunday.noc.nerim.net [178.132.17.100])
        by lindbergh.monkeyblade.net (Postfix) with ESMTP id 9D552EB59
        for <linux-raid@vger.kernel.org>; Sun, 22 Jan 2023 00:52:58 -0800 (PST)
Received: from [192.168.0.252] (plouf.fr.eu.org [213.41.155.166])
        by mallaury.nerim.net (Postfix) with ESMTP id 323FCDB17D;
        Sun, 22 Jan 2023 09:52:48 +0100 (CET)
Message-ID: <b5e7c1bc-0dec-1676-8613-aad2869227e7@plouf.fr.eu.org>
Date:   Sun, 22 Jan 2023 09:52:47 +0100
MIME-Version: 1.0
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101
 Thunderbird/102.6.0
Subject: Re: Transferring an existing system from non-RAID disks to RAID1
 disks in the same computer
To:     H <agents@meddatainc.com>,
        Linux RAID Mailing List <linux-raid@vger.kernel.org>
References: <273d1fc9-853f-a8fa-bb47-2883ba217820@meddatainc.com>
 <deafcb4a-ed1c-d0b3-c9f9-c0a99867bb7a@meddatainc.com>
 <3CEAC9AB-02FC-43BE-94CF-ED3ECFF6F4F7@meddatainc.com>
Content-Language: en-US
From:   Pascal Hambourg <pascal@plouf.fr.eu.org>
Organization: Plouf !
In-Reply-To: <3CEAC9AB-02FC-43BE-94CF-ED3ECFF6F4F7@meddatainc.com>
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Spam-Status: No, score=-2.0 required=5.0 tests=BAYES_00,NICE_REPLY_A,
        RCVD_IN_DNSWL_NONE,SPF_HELO_NONE,SPF_PASS autolearn=ham
        autolearn_force=no version=3.4.6
X-Spam-Checker-Version: SpamAssassin 3.4.6 (2021-04-09) on
        lindbergh.monkeyblade.net
Precedence: bulk
List-ID: <linux-raid.vger.kernel.org>
X-Mailing-List: linux-raid@vger.kernel.org

On 22/01/2023 at 06:05, H wrote:
>>
>> The new one uses two disks, RAID1, LUKS and LVM for everything but
>> /boot and /boot/efi, total of four partitions (swap has its own
>> partition - not sure why I made it that way). A minimal installation of
>> Centos 7 was made to this setup and is working. In other words, UUIDs
>> of disks, partitions and LUKS are already configured and working.
>>
>> So, I am now thinking the following might work:
>>
>> - Make a rsync backup of the new disks to the external harddisk
>> ("BACKUP2").
>>
>> - Delete all files from the new disks except from /boot and /boot/efi.
>>
>> - Copy all files from all partitions except /boot and /boot/efi from
>> BACKUP1 to the new disks. In other words, everything except /boot and
>> /boot/efi will now be overwritten.
>>
>> - I would expect this system not to boot since both /etc/fstab and
>> /etc/crypttab on the new disks contain the UUIDs from the old system.
>>
>> - Copy just /etc/fstab and /etc/crypttab from BACKUP2 to the new disks.
>> This should update the new disks with the previously created UUIDs from
>> when doing the minimal installation of CentOS 7.
>>
>> What do you think?

There are caveats:
- The kernel versions must be the same in the old and new systems so 
that kernel images in /boot and kernel modules in /lib/modules match.
- Make sure that mdadm is installed in the old system. For now it is 
included in the initramfs generated by the new system but if mdadm is 
not installed in the old system, newer initramfs generated by the old 
system will fail to mount the root filesystem.

> I am happy to share that my plan as outlined below worked. I now have
> /boot, /boot/efi and / on separate RAID partitions with the latter
> managed by LVM and encrypted.  All data from the old disk is now on
> the new setup and everything seems to be working.
> 
> However, going back to the issue of /boot/efi possibly not being
> duplicated by CentOS, would not mdadm take care of that automatically?
> How can I check?

Is really /boot/efi on RAID ? You can check with "lsblk".
If it is, can you post the output of

cat /proc/mdstat
fdisk -l
blkid
efibootmgr -v

PS: Your lines are too long.
