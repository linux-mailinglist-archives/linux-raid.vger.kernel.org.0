Return-Path: <linux-raid+bounces-2144-lists+linux-raid=lfdr.de@vger.kernel.org>
X-Original-To: lists+linux-raid@lfdr.de
Delivered-To: lists+linux-raid@lfdr.de
Received: from am.mirrors.kernel.org (am.mirrors.kernel.org [IPv6:2604:1380:4601:e00::3])
	by mail.lfdr.de (Postfix) with ESMTPS id 30FD1929AFE
	for <lists+linux-raid@lfdr.de>; Mon,  8 Jul 2024 05:02:10 +0200 (CEST)
Received: from smtp.subspace.kernel.org (wormhole.subspace.kernel.org [52.25.139.140])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by am.mirrors.kernel.org (Postfix) with ESMTPS id CDFC31F2137F
	for <lists+linux-raid@lfdr.de>; Mon,  8 Jul 2024 03:02:09 +0000 (UTC)
Received: from localhost.localdomain (localhost.localdomain [127.0.0.1])
	by smtp.subspace.kernel.org (Postfix) with ESMTP id DF3F83FC2;
	Mon,  8 Jul 2024 03:02:03 +0000 (UTC)
Authentication-Results: smtp.subspace.kernel.org;
	dkim=pass (2048-bit key) header.d=14.by header.i=@14.by header.b="lPGyj0y7"
X-Original-To: linux-raid@vger.kernel.org
Received: from mail.zeptobars.com (mail.zeptobars.com [144.24.244.140])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by smtp.subspace.kernel.org (Postfix) with ESMTPS id 2F6D94C99
	for <linux-raid@vger.kernel.org>; Mon,  8 Jul 2024 03:02:00 +0000 (UTC)
Authentication-Results: smtp.subspace.kernel.org; arc=none smtp.client-ip=144.24.244.140
ARC-Seal:i=1; a=rsa-sha256; d=subspace.kernel.org; s=arc-20240116;
	t=1720407723; cv=none; b=jEWeYTaAXo1ULKoXsC/BZRRp0OJfuh1VZe3M2nm96DKVJ+72QcsWYsDeCx2aIqZHUJAeJX0TbzHY195UMYl1nC0ZBEUKFm3cs1VoOxCT6xKpaNiMERVK2IwL8VAmnAFp4tfxZckFnRd1tnLo9ZQodTOW5xDgvylfJhT7ClrUjWk=
ARC-Message-Signature:i=1; a=rsa-sha256; d=subspace.kernel.org;
	s=arc-20240116; t=1720407723; c=relaxed/simple;
	bh=X55cPT6csjU1jiU0pjYc718QFXajS7YT9PRzgL8JKhs=;
	h=From:To:Subject:Date:Message-ID:MIME-Version:Content-Type; b=eQphqaEPWOqAzRLhaVwLN79e2a61ebD6vuUhzOjpLfsrYiY0WJMwqOsbwrV9ZOQuwrMvyw2eHNFGCq5OrSmCntudRqcj2f/SxaQG+lZuPlDLaW9zCx7FXCvpnA1Fr8MWl9HhZ9+tsGMTV4rYJPJoVzHetMueq9ehHNn5yi/UG0k=
ARC-Authentication-Results:i=1; smtp.subspace.kernel.org; dmarc=pass (p=quarantine dis=none) header.from=14.by; spf=pass smtp.mailfrom=14.by; dkim=pass (2048-bit key) header.d=14.by header.i=@14.by header.b=lPGyj0y7; arc=none smtp.client-ip=144.24.244.140
Authentication-Results: smtp.subspace.kernel.org; dmarc=pass (p=quarantine dis=none) header.from=14.by
Authentication-Results: smtp.subspace.kernel.org; spf=pass smtp.mailfrom=14.by
Received: from [127.0.0.1] (localhost [127.0.0.1]) by localhost (Mailerdaemon) with ESMTPSA id 4EF4B3F629
	for <linux-raid@vger.kernel.org>; Mon,  8 Jul 2024 05:01:59 +0200 (CEST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=14.by; s=dkim;
	t=1720407719; h=from:subject:date:message-id:to:mime-version:content-type:
	 content-transfer-encoding:content-language;
	bh=X55cPT6csjU1jiU0pjYc718QFXajS7YT9PRzgL8JKhs=;
	b=lPGyj0y7+4Im8uvYhDjku74B59cPFTscamkZopi4Kmp9XLmw0g3zIJ1rw5+UTBm0Ybu1xX
	mnpSPROSWg30Jv1Iu8k3pm/lRgwt0qEvoKMA3hQr3Mrdf0KLvUesccgF1NXKgsB6Gu/mbu
	+J9kE7Md/5QuXYDIuCLBXx532A6/NmNFZZ5CQyQJlqAKruJd9jhBIUJpdjzI0CpoBzuQja
	TOswznegWOW9XD1TW4v2GSooA+CqOSvyYDJ8n6qli98MucGfriDo8lgsNVc7WVVXffAqR2
	fKpRW+6cSbgLD2QSROADT1qCbafM/uJ8FlXRaG+cExNzGZj+ZtG46r73Sblhqw==
From: <3@14.by>
To: <linux-raid@vger.kernel.org>
Subject: raid5/6 error amplification during normal operations
Date: Mon, 8 Jul 2024 05:01:59 +0200
Message-ID: <000f01dad0e3$3308a890$9919f9b0$@14.by>
Precedence: bulk
X-Mailing-List: linux-raid@vger.kernel.org
List-Id: <linux-raid.vger.kernel.org>
List-Subscribe: <mailto:linux-raid+subscribe@vger.kernel.org>
List-Unsubscribe: <mailto:linux-raid+unsubscribe@vger.kernel.org>
MIME-Version: 1.0
Content-Type: text/plain;
	charset="UTF-8"
Content-Transfer-Encoding: quoted-printable
Thread-Index: AdrQ3mg7ZZWqiW/ERamnwa3gt0lSLw==
Content-Language: en-us
X-Last-TLS-Session-Version: TLSv1.2

Hello,=20

As far as I understand, when working with raid5/6 MD driver does not =
check parity during normal read operations. When doing scrubs - when =
parity mismatch is detected, parity data is assumed incorrect and is =
re-regenerated.
Modern HDDs have error read rates ~1 sector per 10^15 bits read, or 1 =
incorrect sector red per ~125Tb of reads.=20
When operating 10x20Tb array in raid6 configuration, during weekly =
scrubs we will read 50'000Tb of data over 5 years of operation.=20

With perfectly working drives and stable ECC RAM we still will get 400 =
incorrect sectors red over 5 years. 20% of these will be in parity =
drives and will be correctly regenerated by MD driver.=20
But unfortunately, in 80% of cases - error will propagate to parity =
drives, and at the end we will get 400*0.8*3=3D960 sectors with =
incorrect data, even though all disks are working on-spec without any =
hardware failures.=20

I.e. while raid-6 does improve tolerance to detectable hardware =
failures, random undetectable errors are actually amplified dramatically =
(due to regular scrubs & 80% chance of copying errors on parity drives).
It means that due to dramatic increase of HDD size over last 20 years - =
intrinsic error rates of HDDs can no longer be ignored. They are as =
serious threat as drive failure.=20

1) It is time to integrate raid6check into MD driver to ensure all =
scrubs are correctly recovering errors. This will reduce raid-6 error =
rate by more than 10^9 (as we will have to randomly get 2 errors while =
reading different drives in the same place which is unlikely). As this =
recovery code only triggers during parity mismatches (which are rare), =
there will be no performance degradation. Right now we just leave data =
integrity on the table and mislead users : many are certain that raid-6 =
can correct random errors, when in reality it does not.=20

2) raid-5/raid-1 will require external checksums in external file (just =
like bitmap) + checksum_actual flag in bitmap to avoid guaranteed =
continuous corruption caused by scrubs and intrinsic error rates of =
HDDs. This will allow MD driver to know which copy of data is correct. =
Checksums can be handled just like write intent bitmap : write first, =
update checksums later at idle time. Running mdadm on top of =
dm-integrity cannot support this delayed checksum update and is =
dramatically slower. 64-bit checksums could also allow experienced users =
to manually recover from single bit flip errors even when there is no =
more parity left.=20

Best regards,
Mikhail


